{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4dZjRLaO60f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "from statsmodels.tsa.stattools import coint\n",
        "from statsmodels.tsa.vector_ar.vecm import VECM\n",
        "from itertools import combinations\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Fetching tickers from Wikipedia pages of S&P500, S&P400, S&P600\n",
        "def get_tickers():\n",
        "    urls = {\n",
        "        \"S&P 500\": \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n",
        "        \"S&P 400\": \"https://en.wikipedia.org/wiki/List_of_S%26P_400_companies\",\n",
        "        \"S&P 600\": \"https://en.wikipedia.org/wiki/List_of_S%26P_600_companies\"\n",
        "    }\n",
        "\n",
        "    tickers = set()  # Use a set to avoid duplicates\n",
        "\n",
        "    for url in urls.values():\n",
        "        df = pd.read_html(url)[0]\n",
        "\n",
        "        # Identify the correct ticker column\n",
        "        ticker_column = [col for col in df.columns if 'Ticker' in col or 'Symbol' in col][0]\n",
        "\n",
        "        # Replace '.' with '-' to match yfinance format\n",
        "        tickers.update(df[ticker_column].str.replace(\".\", \"-\", regex=False).dropna().tolist())\n",
        "\n",
        "    return sorted(tickers)  # Return sorted list of unique tickers\n",
        "\n",
        "\n",
        "def get_log_returns(tickers):\n",
        "    data = yf.download(tickers, period=\"12y\", interval=\"1d\")[\"Close\"]\n",
        "    data = data.ffill().bfill()  # Fill missing values using previous and next available data\n",
        "    log_returns = np.log(data / data.shift(1)).fillna(0)\n",
        "    return log_returns\n",
        "\n",
        "\n",
        "def get_data_from_csv():\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv('/content/1500stockdata.csv', parse_dates=['Date'], index_col='Date', dayfirst=True)\n",
        "\n",
        "    # Calculate log returns for all tickers at once\n",
        "    log_returns = np.log(df / df.shift(1))\n",
        "\n",
        "    # Fill NaN values (from the first row)\n",
        "    log_returns = log_returns.fillna(0)\n",
        "\n",
        "    return log_returns\n",
        "\n",
        "def find_coint_pairs(tickers, log_returns):\n",
        "\n",
        "    sig_level = 0.0005\n",
        "    # Compute correlation matrix\n",
        "    corr_matrix = log_returns.corr()\n",
        "\n",
        "    # Find highly correlated pairs (e.g., correlation > 0.9)\n",
        "    high_coint_pairs = []\n",
        "    threshold = 0.9\n",
        "    upper_threshold = 0.99\n",
        "\n",
        "    for ticker1, ticker2 in combinations(tickers, 2):\n",
        "        if corr_matrix.loc[ticker1, ticker2] > threshold and corr_matrix.loc[ticker1, ticker2] < upper_threshold:  # Only consider pairs that are highly correlated but not perfectly (0.99)\n",
        "            if log_returns[ticker1].std() == 0 or log_returns[ticker2].std() == 0:\n",
        "                continue  # Skip pairs where one stock has zero variance\n",
        "\n",
        "            score, p_value, x = coint(log_returns[ticker1], log_returns[ticker2])\n",
        "\n",
        "            if p_value < sig_level:  # If there is sufficient evidence to suggest the stocks are cointegrated\n",
        "              high_coint_pairs.append([ticker1, ticker2])\n",
        "\n",
        "    return high_coint_pairs\n",
        "\n",
        "\n",
        "# Fit a VECM to find cointegrating relationships\n",
        "def run_vecm(data):\n",
        "    model = VECM(data, k_ar_diff=1, coint_rank=1)  # Rank 1 because we are testing pairs\n",
        "    vecm_fit = model.fit()\n",
        "    residuals = vecm_fit.resid  # Extract error correction term\n",
        "    hedge_ratios = vecm_fit.beta[:, 0]  # First cointegrating vector\n",
        "    return residuals, hedge_ratios\n",
        "\n",
        "# Generates trade signals and calculates profit/loss for a specific year\n",
        "def generate_trade_signals(data, hedge_ratios, year):\n",
        "\n",
        "    # Filter data for the specified year\n",
        "    data = data[data.index.year == year]\n",
        "\n",
        "    # Ensure hedge_ratios is a column vector\n",
        "    hedge_ratios = np.array(hedge_ratios).reshape(-1, 1)\n",
        "\n",
        "    # Compute the spread (stationary component)\n",
        "    spread = data.dot(hedge_ratios)\n",
        "\n",
        "    # Standardize the spread\n",
        "    spread_mean = spread.mean()\n",
        "    spread_std = spread.std()\n",
        "    z_score = (spread - spread_mean) / spread_std\n",
        "\n",
        "    # Define trading signals\n",
        "    long_signal = z_score < -1  # Go long when spread is very negative\n",
        "    short_signal = z_score > 1   # Go short when spread is very positive\n",
        "    exit_signal = abs(z_score) < 0.5  # Exit when spread normalizes\n",
        "\n",
        "    # Construct position dataframe\n",
        "    positions = pd.DataFrame(index=data.index)\n",
        "    positions[data.columns[0]] = long_signal.astype(int) - short_signal.astype(int)\n",
        "    positions[data.columns[1]] = -positions[data.columns[0]]\n",
        "\n",
        "    # Shift positions to avoid look-ahead bias\n",
        "    shifted_positions = positions.shift(1).fillna(0)\n",
        "\n",
        "    # Calculate daily returns based on position and log returns\n",
        "    daily_returns = (shifted_positions * data).sum(axis=1)\n",
        "\n",
        "    # Compute cumulative PnL\n",
        "    pnl = daily_returns.cumsum()\n",
        "\n",
        "    return positions, pnl\n",
        "\n",
        "# Computes annualised_returns, sharpe ratio, max drawdown, volatility and calmar ratio\n",
        "def compute_performance_metrics(cum_pnl):\n",
        "\n",
        "    # Convert cumulative PnL to daily returns\n",
        "    daily_returns = cum_pnl.pct_change().dropna()\n",
        "\n",
        "    # Annualized Return (assuming 252 trading days in a year)\n",
        "    total_return = cum_pnl.iloc[-1] / cum_pnl.iloc[0] - 1\n",
        "    num_years = (cum_pnl.index[-1] - cum_pnl.index[0]).days / 365\n",
        "    annualised_return = (1 + total_return) ** (1 / num_years) - 1\n",
        "\n",
        "    # Sharpe Ratio (assuming risk-free rate is 0)\n",
        "    sharpe_ratio = daily_returns.mean() / daily_returns.std() * np.sqrt(252)\n",
        "\n",
        "    # Maximum Drawdown\n",
        "    rolling_max = cum_pnl.cummax()\n",
        "    drawdown = (cum_pnl - rolling_max) / rolling_max\n",
        "    max_drawdown = drawdown.min()\n",
        "\n",
        "    # Volatility (Annualized Standard Deviation)\n",
        "    volatility = daily_returns.std() * np.sqrt(252)\n",
        "\n",
        "    # Calmar Ratio (Annualized Return / Max Drawdown)\n",
        "    calmar_ratio = annualised_return / abs(max_drawdown) if max_drawdown != 0 else np.nan\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Annualised Return: {annualised_return:.2%}\")\n",
        "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
        "    print(f\"Maximum Drawdown: {max_drawdown:.2%}\")\n",
        "    print(f\"Volatility: {volatility:.2%}\")\n",
        "    print(f\"Calmar Ratio: {calmar_ratio:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"Annualised Return\": annualised_return,\n",
        "        \"Sharpe Ratio\": sharpe_ratio,\n",
        "        \"Maximum Drawdown\": max_drawdown,\n",
        "        \"Volatility\": volatility,\n",
        "        \"Calmar Ratio\": calmar_ratio\n",
        "    }\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    tickers = get_tickers()\n",
        "    log_returns = get_log_returns(tickers)\n",
        "\n",
        "    # Alternative (much quicker) method to get log returns: using pre-downloaded data\n",
        "    # Downloaded data from yfinance onto CSV file (link: https://www.dropbox.com/scl/fi/r5776193mjszok9u1sel9/1500stockdata.csv?rlkey=wj0ux6k8ne2q3klv8vw3rhs35&st=mj8wyx14&dl=0)\n",
        "    # Download the csv then upload when requested\n",
        "    #uploaded = files.upload()\n",
        "    #log_returns = get_data_from_csv()\n",
        "    #log_returns.index = pd.to_datetime(log_returns.index)  # Convert to DateTime\n",
        "    #log_returns = log_returns.asfreq('B')  # Set frequency to business days\n",
        "    #log_returns = log_returns.fillna(0)\n",
        "\n",
        "    log_returns_transpose = log_returns.T # In the format num_stocks x num_dates\n",
        "\n",
        "    tickers = log_returns.columns.tolist()\n",
        "\n",
        "    years = sorted(log_returns.index.year.unique())  # Get unique years in dataset\n",
        "    cum_pnl = pd.Series(dtype=float)  # Start as an empty Series\n",
        "    last_multiplier = 1  # Start at 1 (representing $1)\n",
        "\n",
        "    for year in years:\n",
        "        yearly_pnl = 0  # Initialising yearly_pnl as 0\n",
        "        print(f\"Processing year: {year}\")\n",
        "\n",
        "        # Get previous year's data to find cointegrated pairs\n",
        "        yearly_data = log_returns[log_returns.index.year == year - 1]\n",
        "\n",
        "        # If there's no data for the previous year, skip\n",
        "        if yearly_data.empty:\n",
        "            continue\n",
        "\n",
        "        coint_pairs = find_coint_pairs(tickers, yearly_data)\n",
        "        print(f\"Pairs for {year}: {coint_pairs}\")\n",
        "\n",
        "        # If no cointegrated pairs were found, skip this year\n",
        "        if not coint_pairs:\n",
        "            continue\n",
        "\n",
        "        pair_count = 0 # counting no of coint pairs considered, so we can normalise the yearly_pnl later on\n",
        "\n",
        "        for pair in coint_pairs:\n",
        "            data = yearly_data[pair]  # Get log return data for the pair\n",
        "\n",
        "            # Skip if variance is too low\n",
        "            if np.var(data.values) < 1e-6:\n",
        "                continue\n",
        "\n",
        "            pair_count+=1\n",
        "\n",
        "            residuals, hedge_ratio = run_vecm(data)\n",
        "            # Use only the correct year's data\n",
        "            pos, pnl = generate_trade_signals(log_returns[log_returns.index.year == year][pair], hedge_ratio, year)\n",
        "            yearly_pnl += pnl\n",
        "\n",
        "        yearly_pnl = yearly_pnl/pair_count + 1\n",
        "        yearly_pnl *= last_multiplier\n",
        "\n",
        "        last_multiplier = yearly_pnl.iloc[-1]\n",
        "\n",
        "        if not yearly_pnl.empty:  # Only concatenate if yearly_pnl is not empty\n",
        "            cum_pnl = pd.concat([cum_pnl, yearly_pnl])\n",
        "\n",
        "\n",
        "    # Plot Cumulative PnL\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(cum_pnl.index, cum_pnl, label=\"Cumulative PnL\", color='blue')\n",
        "\n",
        "    # Formatting the plot\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Cumulative PnL\")\n",
        "    plt.title(\"Strategy Cumulative PnL Over Time\")\n",
        "    plt.legend()\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)  # Add a grid for better readability\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    metrics = compute_performance_metrics(cum_pnl) # Print the metrics\n"
      ]
    }
  ]
}